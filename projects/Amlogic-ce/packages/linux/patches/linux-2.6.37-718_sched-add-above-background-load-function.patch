---
 include/linux/sched.h |    7 +++++++
 kernel/sched_bfs.c    |   20 ++++++++++++++++++++
 2 files changed, 27 insertions(+)

Index: linux-2.6.37-ck1/include/linux/sched.h
===================================================================
--- linux-2.6.37-ck1.orig/include/linux/sched.h	2011-01-06 14:07:00.000000000 +1100
+++ linux-2.6.37-ck1/include/linux/sched.h	2011-01-06 14:07:19.971048973 +1100
@@ -1558,6 +1558,7 @@ static inline int iso_task(struct task_s
 	return (p->policy == SCHED_ISO);
 }
 extern void remove_cpu(unsigned long cpu);
+extern int above_background_load(void);
 #else /* CFS */
 extern int runqueue_is_locked(int cpu);
 #define tsk_seruntime(t)	((t)->se.sum_exec_runtime)
@@ -1581,6 +1582,12 @@ static inline int iso_task(struct task_s
 static inline void remove_cpu(unsigned long cpu)
 {
 }
+
+/* Anyone feel like implementing this? */
+static inline int above_background_load(void)
+{
+	return 1;
+}
 #endif /* CONFIG_SCHED_BFS */
 
 /* Future-safe accessor for struct task_struct's cpus_allowed. */
Index: linux-2.6.37-ck1/kernel/sched_bfs.c
===================================================================
--- linux-2.6.37-ck1.orig/kernel/sched_bfs.c	2011-01-06 14:07:00.000000000 +1100
+++ linux-2.6.37-ck1/kernel/sched_bfs.c	2011-01-06 14:07:19.972049048 +1100
@@ -559,6 +559,26 @@ static inline void __task_grq_unlock(voi
 	grq_unlock();
 }
 
+/*
+ * Look for any tasks *anywhere* that are running nice 0 or better. We do
+ * this lockless for overhead reasons since the occasional wrong result
+ * is harmless.
+ */
+int above_background_load(void)
+{
+	struct task_struct *cpu_curr;
+	unsigned long cpu;
+
+	for_each_online_cpu(cpu) {
+		cpu_curr = cpu_rq(cpu)->curr;
+		if (unlikely(!cpu_curr))
+			continue;
+		if (PRIO_TO_NICE(cpu_curr->static_prio) < 1)
+			return 1;
+	}
+	return 0;
+}
+
 #ifndef __ARCH_WANT_UNLOCKED_CTXSW
 static inline void prepare_lock_switch(struct rq *rq, struct task_struct *next)
 {
