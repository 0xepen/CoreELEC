---
 include/linux/sched.h |    7 +++++++
 kernel/sched_bfs.c    |   20 ++++++++++++++++++++
 2 files changed, 27 insertions(+)

Index: linux-2.6.37-ck2/include/linux/sched.h
===================================================================
--- linux-2.6.37-ck2.orig/include/linux/sched.h	2011-02-14 09:47:50.988252000 +1100
+++ linux-2.6.37-ck2/include/linux/sched.h	2011-02-14 10:11:00.292251999 +1100
@@ -1558,6 +1558,7 @@
 	return (p->policy == SCHED_ISO);
 }
 extern void remove_cpu(unsigned long cpu);
+extern int above_background_load(void);
 #else /* CFS */
 extern int runqueue_is_locked(int cpu);
 #define tsk_seruntime(t)	((t)->se.sum_exec_runtime)
@@ -1581,6 +1582,12 @@
 static inline void remove_cpu(unsigned long cpu)
 {
 }
+
+/* Anyone feel like implementing this? */
+static inline int above_background_load(void)
+{
+	return 1;
+}
 #endif /* CONFIG_SCHED_BFS */
 
 /* Future-safe accessor for struct task_struct's cpus_allowed. */
Index: linux-2.6.37-ck2/kernel/sched_bfs.c
===================================================================
--- linux-2.6.37-ck2.orig/kernel/sched_bfs.c	2011-02-14 09:53:53.820252000 +1100
+++ linux-2.6.37-ck2/kernel/sched_bfs.c	2011-02-14 10:11:00.294252001 +1100
@@ -559,6 +559,26 @@
 	grq_unlock();
 }
 
+/*
+ * Look for any tasks *anywhere* that are running nice 0 or better. We do
+ * this lockless for overhead reasons since the occasional wrong result
+ * is harmless.
+ */
+int above_background_load(void)
+{
+	struct task_struct *cpu_curr;
+	unsigned long cpu;
+
+	for_each_online_cpu(cpu) {
+		cpu_curr = cpu_rq(cpu)->curr;
+		if (unlikely(!cpu_curr))
+			continue;
+		if (PRIO_TO_NICE(cpu_curr->static_prio) < 1)
+			return 1;
+	}
+	return 0;
+}
+
 #ifndef __ARCH_WANT_UNLOCKED_CTXSW
 static inline void prepare_lock_switch(struct rq *rq, struct task_struct *next)
 {
